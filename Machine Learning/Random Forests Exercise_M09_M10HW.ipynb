{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1**\n",
        "\n",
        "The “recursive” part of the recursive binary splitting algorithm means that the algorithm repeatedly divides the dataset into smaller and smaller subsets. It does this by choosing a single predictor at each step, creating a decision point or “split” to separate the data. The “binary” aspect refers to each split producing exactly two child nodes, each representing a subset of the original data based on whether a given condition is met. In the case of our dataset, recursive binary splitting would involve evaluating potential splits based on each level of x1 and different thresholds of x2, iteratively dividing the data based on which splits reduce node impurity the most. Node impurity measures how mixed a node is, and the goal of splitting is to decrease impurity in the child nodes. For classification trees, impurity is usually measured using metrics which quantify the diversity of classes within a node. In regression trees, impurity is measured using metrics like the sum of squared deviations, focusing on the spread of quantitative outcomes in each node. The algorithm continues  until nodes reach a minimum impurity or size, resulting in a tree structure that organizes the data based on the chosen splits."
      ],
      "metadata": {
        "id": "DnaUbtwV0zAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**\n",
        "\n",
        "Tuning parameters for a decision tree include max depth, min samples split, and min samples leaf. Choosing a larger max depth or lower minimum samples can lead to a model that closely fits the training data, which reduces bias but increases variance, potentially causing overfitting. On the other hand, limiting depth or increasing the minimum samples can prevent overfitting, leading to higher bias but reduced variance, which may help the model generalize better on unseen data."
      ],
      "metadata": {
        "id": "DjDKeTq3fUY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Question 3**\n",
        "\n",
        "Variable importance in decision trees is determined by how much each feature contributes to reducing impurity at each split. Features that make more significant splits, leading to greater decreases in impurity, are assigned higher importance scores, as they help the model make more accurate predictions. This measure helps identify which variables have the most impact on the model's predictions, which can be useful for feature selection and understanding the relationships within the data."
      ],
      "metadata": {
        "id": "duguzA_KfXNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**\n",
        "\n",
        "Bagging, random forests, and boosting improve decision tree performance by addressing overfitting and enhancing predictive accuracy. Bagging creates multiple trees from different samples of the data and averages their predictions, reducing variance. Random forests add further randomness by selecting a subset of features for each split, enhancing robustness. Finally, boosting sequentially adjusts trees to correct previous errors, reducing bias and producing a more accurate model."
      ],
      "metadata": {
        "id": "910FDnX0fbOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5**\n",
        "\n",
        "\n",
        "For bagging, the primary tuning parameter is the number of trees, which affects variance reduction. In random forests, key parameters include the number of trees and the number of features considered at each split, balancing randomness and model robustness. For boosting, parameters like the learning rate, number of trees, and tree depth control how aggressively errors are corrected, impacting both bias and variance in the final model."
      ],
      "metadata": {
        "id": "QSgCtRIBfcud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 6**"
      ],
      "metadata": {
        "id": "hcZqEF96fL9l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "collapsed": true,
        "id": "kj8Zq2n50woa",
        "outputId": "74ad7623-95ee-4f3c-fca5-211e7c742e1e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9e9f29d2-06d5-4aed-b8e2-44700bb47538\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9e9f29d2-06d5-4aed-b8e2-44700bb47538\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the file into a pandas dataframe\n",
        "df = pd.read_csv(io.BytesIO(uploaded['acs12.csv']))\n",
        "\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "  df[col] = df[col].astype('category')\n",
        "\n",
        "print(df)\n",
        "!pip install ISLP"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)"
      ],
      "metadata": {
        "id": "zUkeTzFFe9I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib.pyplot import subplots\n",
        "from statsmodels.datasets import get_rdataset\n",
        "import sklearn.model_selection as skm\n",
        "from ISLP import load_data, confusion_table\n",
        "from ISLP.models import ModelSpec as MS\n",
        "from sklearn.tree import (DecisionTreeClassifier as DTC,\n",
        "  DecisionTreeRegressor as DTR,\n",
        "  plot_tree,\n",
        "  export_text)\n",
        "from sklearn.metrics import (accuracy_score,\n",
        "  log_loss)\n",
        "from sklearn.ensemble import \\\n",
        "  (RandomForestClassifier as RF,\n",
        "  GradientBoostingClassifier as GBC)\n",
        "from ISLP.bart import BART\n",
        "\n",
        "# Create binary variable from 'income'\n",
        "median_income = df['income'].median()\n",
        "high = np.where(df['income'] > median_income, 'Yes', 'No')\n",
        "\n",
        "# Fit classification tree\n",
        "model = MS(df.columns.drop('income'), intercept=False)\n",
        "D = model.fit_transform(df)\n",
        "feature_names = list(D.columns)\n",
        "X = np.asarray(D)\n",
        "\n",
        "clf = DTC(criterion='entropy',\n",
        "  max_depth=3,\n",
        "  random_state=0)\n",
        "print(clf.fit(X, high))\n",
        "\n",
        "# Accuracy score\n",
        "print(accuracy_score(high, clf.predict(X)))\n",
        "\n",
        "# Deviance value\n",
        "resid_dev = np.sum(log_loss(high, clf.predict_proba(X)))\n",
        "print(resid_dev)\n",
        "\n",
        "# Plot\n",
        "ax = subplots(figsize=(12,12))[1]\n",
        "plot_tree(clf,\n",
        "feature_names=feature_names,\n",
        "ax=ax);\n",
        "\n",
        "print(export_text(clf,\n",
        "  feature_names=feature_names,\n",
        "  show_weights=True))\n",
        "\n",
        "# Estimate test error\n",
        "validation = skm.ShuffleSplit(n_splits=1,\n",
        "  test_size=200,\n",
        "  random_state=0)\n",
        "results = skm.cross_validate(clf,\n",
        "  D,\n",
        "  high,\n",
        "  cv=validation)\n",
        "print(results['test_score'])\n",
        "\n",
        "# Split data\n",
        "(X_train,\n",
        "  X_test,\n",
        "  High_train,\n",
        "  High_test) = skm.train_test_split(X,\n",
        "    high,\n",
        "    test_size=0.5,\n",
        "    random_state=0)\n",
        "\n",
        "# Refit tree on training set\n",
        "clf = DTC(criterion='entropy', random_state=0)\n",
        "clf.fit(X_train, High_train)\n",
        "print(accuracy_score(High_test, clf.predict(X_test)))\n",
        "\n",
        "# Extract cost-complexity values\n",
        "ccp_path = clf.cost_complexity_pruning_path(X_train, High_train)\n",
        "kfold = skm.KFold(10,\n",
        "  random_state=1,\n",
        "  shuffle=True)\n",
        "\n",
        "# Extract optimal through cross-validation\n",
        "grid = skm.GridSearchCV(clf,\n",
        "  {'ccp_alpha': ccp_path.ccp_alphas},\n",
        "  refit=True,\n",
        "  cv=kfold,\n",
        "  scoring='accuracy')\n",
        "grid.fit(X_train, High_train)\n",
        "print(grid.best_score_)\n",
        "\n",
        "# Plot pruned\n",
        "ax = subplots(figsize=(12, 12))[1]\n",
        "best_ = grid.best_estimator_\n",
        "plot_tree(best_,\n",
        "  feature_names=feature_names,\n",
        "  ax=ax);\n",
        "\n",
        "# Query best\n",
        "print(best_.tree_.n_leaves)\n",
        "\n",
        "# Fit pruned tree on test set\n",
        "print(accuracy_score(High_test,\n",
        "best_.predict(X_test)))\n",
        "confusion = confusion_table(best_.predict(X_test),\n",
        "High_test)\n",
        "print(confusion)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "txVHKIsIez9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)"
      ],
      "metadata": {
        "id": "aj-39SiQlm-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# n=100\n",
        "bagging_model1 = RF(n_estimators=100, max_features=X_train.shape[1], random_state=0)\n",
        "bagging_model1.fit(X_train, High_train)\n",
        "\n",
        "# Prediction on test set\n",
        "y_hat_bagging1 = bagging_model1.predict(X_test)\n",
        "bagging_accuracy1 = accuracy_score(High_test, y_hat_bagging1)\n",
        "print(bagging_accuracy1)\n",
        "\n",
        "# Confusion matrix\n",
        "bag_conf_matrix1 = confusion_matrix(High_test, y_hat_bagging1)\n",
        "print(bag_conf_matrix1)\n",
        "\n",
        "# Feature importance\n",
        "bag_feature_imp1 = pd.DataFrame({\n",
        "    'importance': bagging_model1.feature_importances_},\n",
        "    index=feature_names)\n",
        "print(bag_feature_imp1.sort_values(by='importance', ascending=False))\n",
        "\n",
        "\n",
        "# n=200\n",
        "bagging_model2 = RF(n_estimators=200, max_features=X_train.shape[1], random_state=0)\n",
        "bagging_model2.fit(X_train, High_train)\n",
        "\n",
        "# Prediction on test set\n",
        "y_hat_bagging2 = bagging_model2.predict(X_test)\n",
        "bagging_accuracy2 = accuracy_score(High_test, y_hat_bagging2)\n",
        "print(bagging_accuracy2)\n",
        "\n",
        "# Confusion matrix\n",
        "bag_conf_matrix2 = confusion_matrix(High_test, y_hat_bagging2)\n",
        "print(bag_conf_matrix2)\n",
        "\n",
        "# Feature importance\n",
        "bag_feature_imp2 = pd.DataFrame({\n",
        "    'importance': bagging_model2.feature_importances_},\n",
        "    index=feature_names)\n",
        "print(bag_feature_imp2.sort_values(by='importance', ascending=False))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fYqXSAFSsmTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)"
      ],
      "metadata": {
        "id": "tMKfh1PzqP_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# n=100\n",
        "random_forest_model1 = RF(n_estimators=100, random_state=0)\n",
        "random_forest_model1.fit(X_train, High_train)\n",
        "\n",
        "# Prediction on test set\n",
        "y_hat_random_forest1 = random_forest_model1.predict(X_test)\n",
        "random_forest_accuracy1 = accuracy_score(High_test, y_hat_random_forest1)\n",
        "print(random_forest_accuracy1)\n",
        "\n",
        "# Confusion matrix\n",
        "rf_conf_matrix1 = confusion_matrix(High_test, y_hat_random_forest1)\n",
        "print(rf_conf_matrix1)\n",
        "\n",
        "# Feature importance\n",
        "rf_feature_imp1 = pd.DataFrame({\n",
        "    'importance': random_forest_model1.feature_importances_},\n",
        "    index=feature_names)\n",
        "print(rf_feature_imp1.sort_values(by='importance', ascending=False))\n",
        "\n",
        "\n",
        "# n=200\n",
        "random_forest_model2 = RF(n_estimators=200, random_state=0)\n",
        "random_forest_model2.fit(X_train, High_train)\n",
        "\n",
        "# Prediction on test set\n",
        "y_hat_random_forest2 = random_forest_model2.predict(X_test)\n",
        "random_forest_accuracy2 = accuracy_score(High_test, y_hat_random_forest2)\n",
        "print(random_forest_accuracy2)\n",
        "\n",
        "# Confusion matrix\n",
        "rf_conf_matrix2 = confusion_matrix(High_test, y_hat_random_forest2)\n",
        "print(rf_conf_matrix2)\n",
        "\n",
        "# Feature importance\n",
        "rf_feature_imp2 = pd.DataFrame({\n",
        "    'importance': random_forest_model2.feature_importances_},\n",
        "    index=feature_names)\n",
        "print(rf_feature_imp2.sort_values(by='importance', ascending=False))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1_sqCg9fqQ8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4)"
      ],
      "metadata": {
        "id": "-E2k8yxTsBaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit boost model (learning rate = 0.001)\n",
        "boost_model1 = GBC(n_estimators=5000,\n",
        "  learning_rate=0.001,\n",
        "  max_depth=1,\n",
        "  random_state=0)\n",
        "boost_model1.fit(X_train, High_train)\n",
        "\n",
        "# Accuracy\n",
        "accuracy1 = np.zeros_like(boost_model1.train_score_)\n",
        "for idx, y_ in enumerate(boost_model1.staged_predict(X_test)):\n",
        "  accuracy1[idx] = 1.0 - accuracy_score(High_test, y_)\n",
        "\n",
        "# Plot\n",
        "plot_idx = np.arange(boost_model1.train_score_.shape[0])\n",
        "ax = subplots(figsize=(8,8))[1]\n",
        "ax.plot(plot_idx,\n",
        "  boost_model1.train_score_,\n",
        "  'b',\n",
        "  label='Training')\n",
        "ax.plot(plot_idx,\n",
        "  accuracy1,\n",
        "  'r',\n",
        "  label='Test')\n",
        "ax.legend();\n",
        "\n",
        "# Predictions\n",
        "y_hat_boost = boost_model1.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "boost_accuracy1 = accuracy_score(High_test, y_hat_boost)\n",
        "print(boost_accuracy1)\n",
        "\n",
        "# Confusion matrix\n",
        "boost_conf_matrix1 = confusion_matrix(High_test, y_hat_boost)\n",
        "print(boost_conf_matrix1)\n",
        "\n",
        "\n",
        "# Fit boost model (learning rate = 0.01)\n",
        "boost_model2 = GBC(n_estimators=5000,\n",
        "  learning_rate=0.01,\n",
        "  max_depth=1,\n",
        "  random_state=0)\n",
        "boost_model2.fit(X_train, High_train)\n",
        "\n",
        "# Accuracy\n",
        "accuracy2 = np.zeros_like(boost_model2.train_score_)\n",
        "for idx, y_ in enumerate(boost_model2.staged_predict(X_test)):\n",
        "  accuracy2[idx] = 1.0 - accuracy_score(High_test, y_)\n",
        "\n",
        "# Plot\n",
        "plot_idx = np.arange(boost_model2.train_score_.shape[0])\n",
        "ax = subplots(figsize=(8,8))[1]\n",
        "ax.plot(plot_idx,\n",
        "  boost_model2.train_score_,\n",
        "  'b',\n",
        "  label='Training')\n",
        "ax.plot(plot_idx,\n",
        "  accuracy2,\n",
        "  'r',\n",
        "  label='Test')\n",
        "ax.legend();\n",
        "\n",
        "# Predictions\n",
        "y_hat_boost2 = boost_model2.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "boost_accuracy2 = accuracy_score(High_test, y_hat_boost2)\n",
        "print(boost_accuracy2)\n",
        "\n",
        "# Confusion matrix\n",
        "boost_conf_matrix2 = confusion_matrix(High_test, y_hat_boost2)\n",
        "print(boost_conf_matrix2)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RGp-MlcAsDOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To classify individuals as having an income above or below the median, I began by building a basic classification tree. Using entropy as the criteria and tuning with cost-complexity pruning, I identified the best tree depth that balanced model complexity and accuracy. The pruned tree showed \"hours worked\" and \"education level\" as primary predictors, suggesting that individuals with more work hours or higher education are more likely to have above-median income. The model had a training accuracy of approximately 72.7% and was further validated with a test accuracy of 77.8%. Key nodes in the pruned tree indicated that marital status, age, and gender also contribute meaningfully to income classification, highlighting potential socioeconomic factors influencing income levels.\n",
        "\n",
        "I then used a few other methods to improve predictive accuracy. For bagging, I created models with 100 and 200 trees, resulting in test accuracies of 77.3% and 76.8%, respectively, showing that increasing the number of trees led to minimal improvement. Moving to random forests, I tested models with 100 and 200 trees, which yielded similar results but with increased interpretability through feature importance analysis. The top predictors were \"age,\" \"hours worked,\" and \"time to work,\" which aligns with expectations about income determinants. Finally, I used boosting with learning rates of 0.001 and 0.01. The model with a lower learning rate achieved slightly higher accuracy (78.3%) than the faster learning rate (76.3%), indicating a trade-off between convergence speed and accuracy. These results suggest that ensemble methods, particularly boosting, can enhance prediction accuracy, though the impact of additional trees or a faster learning rate was limited."
      ],
      "metadata": {
        "id": "du-4_aeZDz0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References**\n",
        "\n",
        "Used Google Colab's built-in Gemini feature to help change the code for MSE (numeric) in the ISLP boosting example to accuracy (categorical)"
      ],
      "metadata": {
        "id": "6ZI8tkAOwWcQ"
      }
    }
  ]
}